system:
  gpu_device: "cuda:0"
  max_vram_usage: 11.5  # GB (1GB reserve for other containers)
  shared_gpu: true  # Proxmox LXC - VRAM shared with Ollama etc.
  
single_worker:
  lazy_loading: true  # Load models only when needed
  model_timeout_minutes: 5  # Unload model after 5min of inactivity (like Ollama)
  
# Main model configuration - Change here to switch between large-v2 and large-v3
model:
  default_model: "large-v3"  # Options: "large-v2", "large-v3"
  vram_usage_gb: 4.7        # VRAM usage for large models
  batch_size: 24             # Optimal batch size for large models
  expected_throughput: 3000 # Expected files per hour
  
# Concurrent processing for GPU optimization
concurrent_processing:
  enabled: true              # Enable concurrent file processing
  max_concurrent_files: 20   # Max files processed simultaneously (auto-adjusted per GPU)
  adaptive_batching: true    # Dynamically adjust based on file sizes
  vram_threshold: 0.9       # Use up to 90% of available VRAM (aggressive)
  auto_gpu_scaling: true     # Automatically optimize settings per GPU model
  
  # VRAM calculation parameters (fine-tune if needed)
  base_model_vram_gb: 4.5    # Base Whisper model VRAM usage (reduced from 4.7)
  overhead_per_file_gb: 0.25 # VRAM overhead per concurrent file (reduced from 0.3)
  safety_buffer_gb: 0.3     # Safety buffer to prevent OOM (reduced from 0.5)
  
  # Dynamic VRAM optimization during runtime
  dynamic_vram_scaling: true # Enable real-time VRAM-based scaling
  target_vram_min: 0.75     # Target minimum VRAM usage (75% - aggressive)
  target_vram_max: 0.85     # Target maximum VRAM usage (85% - push the limits)
  emergency_vram_max: 0.90  # Emergency threshold - force scale down (90% - safety)
  scaling_step_size: 1      # How many files to add/remove per adjustment (fine control)
  scaling_check_interval: 1 # Check VRAM every single file for rapid scaling
  scaling_time_interval: 30 # Also check VRAM every 30 seconds regardless of file completion

# Language settings
language:
  default_language: "de"    # Default language for transcription
  auto_detect: true         # Enable automatic language detection

# Output settings
output:
  directory: "./output"     # Output directory for transcripts and metrics (relative to repo root)

api:
  host: "0.0.0.0"
  port: 8000
  max_batch_size: 100
  
monitoring:
  enable_basic_logging: true
  log_level: "INFO"
